{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc7eb7b",
   "metadata": {},
   "source": [
    "# Code Based Grading\n",
    "\n",
    "## Key Concepts\n",
    "- Validates format and syntax of AI-generated code (Python, JSON, Regex)\n",
    "- Two validation aspects: correct format (code only, no explanations) + valid syntax\n",
    "- Syntax validators return 10 for valid, 0 for invalid\n",
    "- Dataset requires \"format\" field to specify expected output type\n",
    "- Final score combines model grader + syntax validator (averaged)\n",
    "- Enables quantitative measurement of prompt engineering progress\n",
    "\n",
    "## Important Code Patterns\n",
    "- `validate_json(text)` - try json.loads(), return 10 if success, 0 if JSONDecodeError\n",
    "- `validate_python(text)` - try ast.parse(), return 10 if success, 0 if SyntaxError  \n",
    "- `validate_regex(text)` - try re.compile(), return 10 if success, 0 if re.error\n",
    "- `grade_syntax(output, test_case)` - route to appropriate validator based on test_case[\"format\"]\n",
    "- `score = (model_score + syntax_score) / 2` - combine both grader types\n",
    "- Pre-fill with `\"```code\"` instead of language-specific markers\n",
    "- Update dataset generation to include format field in test cases\n",
    "\n",
    "## Best Practices\n",
    "- Use try/except blocks for syntax validation with specific error types\n",
    "- Strip whitespace before parsing (text.strip())\n",
    "- Add clear prompt instructions: \"respond only with code, no comments or explanations\"\n",
    "- Dataset format field enables routing to correct validator\n",
    "- Equal weighting (average) for model and syntax scores is good starting point\n",
    "- Adjust score weights based on what matters more for your use case\n",
    "- Baseline score enables objective tracking of prompt improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5437be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-haiku-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e788701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a new dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "438ed743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and write it to 'dataset.json'\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b89174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83809a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7953c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bcc4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa99d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30fae983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8.166666666666666\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbcc6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\n\\ndef extract_aws_region(s3_url):\\n    match = re.search(r's3://[^.]+\\\\.([^.]+)\\\\.amazonaws\\\\.com', s3_url)\\n    if match:\\n        return match.group(1)\\n    return None\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Extract the AWS region from an S3 bucket URL in the format 's3://bucket-name.region.amazonaws.com/key'\",\n",
      "      \"format\": \"regex\"\n",
      "    },\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The solution correctly implements the specific format requested in the task. However, it's brittle and assumes a single URL format without validation. Real-world S3 URLs come in multiple formats (virtual-hosted, path-style, S3 Transfer Acceleration, etc.), and the function lacks robustness for edge cases. The code would benefit from input validation and clearer documentation of its scope.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport json\\nimport re\\nfrom datetime import datetime\\n\\ndef parse_cloudwatch_log(log_entry):\\n    # AWS CloudWatch log format: timestamp level message\\n    pattern = r'^(\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}Z)\\\\s+(\\\\w+)\\\\s+(.*)$'\\n    \\n    match = re.match(pattern, log_entry)\\n    \\n    if match:\\n        return {\\n            \\\"timestamp\\\": match.group(1),\\n            \\\"log_level\\\": match.group(2),\\n            \\\"message\\\": match.group(3)\\n        }\\n    return None\\n\\n# Example usage\\nlog_entry = \\\"2023-10-15T14:30:45.123Z INFO Application started successfully\\\"\\nresult = parse_cloudwatch_log(log_entry)\\nprint(json.dumps(result, indent=2))\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Parse an AWS CloudWatch log entry and extract the timestamp, log level, and message\",\n",
      "      \"format\": \"python\"\n",
      "    },\n",
      "    \"score\": 7.5,\n",
      "    \"reasoning\": \"The solution adequately handles the basic example provided but lacks robustness for real-world CloudWatch logs. AWS CloudWatch produces diverse log formats (JSON, structured logs, multi-line errors, variable spacing), and this solution only works for one specific format. The lack of error handling and input validation makes it unsuitable for production use. For a task focused on AWS expertise, the solution should demonstrate awareness of CloudWatch's actual log format variations and include defensive programming practices.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"Version\\\": \\\"2012-10-17\\\",\\n  \\\"Statement\\\": [\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": [\\n        \\\"s3:GetObject\\\",\\n        \\\"s3:GetObjectVersion\\\",\\n        \\\"s3:ListBucket\\\",\\n        \\\"s3:GetBucketVersioning\\\",\\n        \\\"s3:GetBucketLocation\\\"\\n      ],\\n      \\\"Resource\\\": [\\n        \\\"arn:aws:s3:::my-bucket\\\",\\n        \\\"arn:aws:s3:::my-bucket/*\\\"\\n      ]\\n    }\\n  ]\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON IAM policy that allows read-only access to a specific S3 bucket\",\n",
      "      \"format\": \"json\"\n",
      "    },\n",
      "    \"score\": 9.0,\n",
      "    \"reasoning\": \"The policy effectively grants read-only S3 access with appropriate actions and correctly structured ARNs. The main issues are minor: the placeholder bucket name requires manual replacement, and one action (GetBucketVersioning) is somewhat optional depending on use case. The policy is functionally sound and follows AWS best practices for least privilege, but could be more polished with clearer guidance on customization.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
